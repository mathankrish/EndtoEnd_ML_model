# -*- coding: utf-8 -*-
"""credit_card_fraud_detection_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OuwxONMLvYodCCeUTjTmJjSkLIFUcoSH
"""

# import necessary packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, f1_score, precision_recall_curve, average_precision_score
from sklearn import metrics
from google.cloud import storage
from joblib import dump
from sklearn.model_selection import train_test_split

def load_data(filename):
  df = pd.read_csv(filename)
  return df

def preprocess(df):
  # Apply label encoding to all columns
  df = df.apply(LabelEncoder().fit_transform)
  x = df.drop("isFraud", axis = 1)
  y = df["isFraud"]
  return x, y

def train_model(x_train, y_train):
    model = RandomForestClassifier()
    pipeline = make_pipeline(model)
    pipeline.fit(x_train, y_train)
    return pipeline

def evaluate_model(model, X_train, y_train, X_test, y_test):
    """
    Evaluate a machine learning model using various metrics and visualizations.

    Parameters:
    - model: The trained machine learning model.
    - X_train, y_train: Training data features and labels.
    - X_test, y_test: Test data features and labels.

    Returns:
    - None
    """

    # start_time = time.time()
    eval_stats = {}

    fig, axs = plt.subplots(1, 3, figsize=(12, 5))

    for type, features, target in (('train', X_train, y_train),
                                   ('test', X_test, y_test)):

        eval_stats[type] = {}

        pred_target = model.predict(features)
        pred_proba = model.predict_proba(features)[:, 1]

        # F1 Score
        f1_thresholds = np.arange(0, 1.01, 0.05)
        f1_scores = [metrics.f1_score(target, pred_proba >= threshold) for threshold in f1_thresholds]

        # ROC Curve
        fpr, tpr, roc_thresholds = metrics.roc_curve(target, pred_proba)
        roc_auc = metrics.roc_auc_score(target, pred_proba)
        eval_stats[type]['ROC AUC'] = roc_auc

        # Precision-Recall Curve
        precision, recall, pr_thresholds = metrics.precision_recall_curve(target, pred_proba)
        aps = metrics.average_precision_score(target, pred_proba)
        eval_stats[type]['APS'] = aps

        # Plotting F1 Score
        ax = axs[0]
        max_f1_score_idx = np.argmax(f1_scores)
        ax.plot(f1_thresholds, f1_scores,
                label=f'{type}, max={f1_scores[max_f1_score_idx]:.2f} @ {f1_thresholds[max_f1_score_idx]:.2f}')
        ax.scatter(f1_thresholds[max_f1_score_idx], f1_scores[max_f1_score_idx], color='red')
        ax.set_xlim([-0.02, 1.02])
        ax.set_ylim([-0.02, 1.02])
        ax.set_xlabel('Threshold')
        ax.set_ylabel('F1 Score')
        ax.legend(loc='lower center')
        ax.set_title('F1 Score')

        # Plotting ROC Curve
        ax = axs[1]
        ax.plot(fpr, tpr, label=f'{type}, ROC AUC={roc_auc:.2f}')
        ax.plot([0, 1], [0, 1], linestyle='--', color='grey')
        ax.scatter(fpr[1], tpr[1], color='red')
        ax.set_xlim([-0.02, 1.02])
        ax.set_ylim([-0.02, 1.02])
        ax.set_xlabel('False Positive Rate')
        ax.set_ylabel('True Positive Rate')
        ax.legend(loc='lower center')
        ax.set_title('ROC Curve')

        # Plotting Precision-Recall Curve
        ax = axs[2]
        ax.plot(recall, precision, label=f'{type}, APS={aps:.2f}')
        ax.scatter(recall[np.argmax(precision)], precision[np.argmax(precision)], color='red')
        ax.set_xlim([-0.02, 1.02])
        ax.set_ylim([-0.02, 1.02])
        ax.set_xlabel('Recall')
        ax.set_ylabel('Precision')
        ax.legend(loc='lower center')
        ax.set_title('Precision-Recall Curve')

        eval_stats[type]['Accuracy'] = metrics.accuracy_score(target, pred_target)
        eval_stats[type]['F1'] = metrics.f1_score(target, pred_target)

    df_eval_stats = pd.DataFrame(eval_stats).round(2)
    df_eval_stats = df_eval_stats.reindex(index=('Accuracy', 'F1', 'APS', 'ROC AUC'))

    # end_time = time.time()
    # print(f"Time: {end_time - start_time:.2f} seconds")
    print(df_eval_stats)

    return

# Use the below code for the gcp

# def save_model_artifact(pipeline):
#     artifact_name = 'model.joblib'
#     dump(pipeline, artifact_name)
#     model_artifact = bucket.blob('bike-share-rf-regression-artifact/'+artifact_name)
#     model_artifact.upload_from_filename(artifact_name)


def save_model(model):
  dump(model, "credit_model.joblib")

path = "/content/credit-card-fraud-detection-data.csv"
df = load_data(path)
x, y = preprocess(df)
input_median = SimpleImputer(missing_values=np.nan, strategy ='median')
x1 = input_median.fit_transform(x)
xtrain, xtest, ytrain, ytest = train_test_split(x1, y, test_size = 0.2, random_state = 10, stratify = y)
model = train_model(xtrain, ytrain)
ypred = model.predict(xtest)
print(accuracy_score(ytest, ypred))
evaluate_model(model, xtrain, ytrain, xtest, ytest)

save_model(model)

